[
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Show R Code\nx &lt;- c(1,3,2,5)\nx\n\n\n[1] 1 3 2 5\n\n\nShow R Code\nx = c(1,6,2)\nx\n\n\n[1] 1 6 2\n\n\nShow R Code\ny = c(1,4,3)\n\n\n\n\n\n\n\nShow R Code\nlength(x)  # What does length() do?\n\n\n[1] 3\n\n\nShow R Code\nlength(y)\n\n\n[1] 3\n\n\n\n\n\n\n\nShow R Code\nx+y\n\n\n[1]  2 10  5\n\n\nShow R Code\nls() # List objects in the environment\n\n\n[1] \"x\" \"y\"\n\n\nShow R Code\nrm(x,y) # Remove objects\nls()\n\n\ncharacter(0)\n\n\nShow R Code\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n\n\nShow R Code\n?matrix\n\n\nstarting httpd help server ... done\n\n\nShow R Code\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nShow R Code\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nShow R Code\nsqrt(x) # What does x look like?\n\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\n\nShow R Code\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nShow R Code\nx^2\n\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nShow R Code\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n\n[1] 0.9930729\n\n\nShow R Code\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nShow R Code\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\n\n\nShow R Code\nmean(y)\n\n\n[1] 0.01103557\n\n\nShow R Code\nvar(y)\n\n\n[1] 0.7328675\n\n\nShow R Code\nsqrt(var(y))\n\n\n[1] 0.8560768\n\n\nShow R Code\nsd(y)\n\n\n[1] 0.8560768\n\n\n\n\n\n\n\nShow R Code\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\n\n\nShow R Code\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\n\n\nShow R Code\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\n\n\nShow R Code\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\n\npng \n  2 \n\n\nShow R Code\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nShow R Code\nx=1:10\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nShow R Code\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Show R Code\nx &lt;- c(1,3,2,5)\nx\n\n\n[1] 1 3 2 5\n\n\nShow R Code\nx = c(1,6,2)\nx\n\n\n[1] 1 6 2\n\n\nShow R Code\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Show R Code\nlength(x)  # What does length() do?\n\n\n[1] 3\n\n\nShow R Code\nlength(y)\n\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Show R Code\nx+y\n\n\n[1]  2 10  5\n\n\nShow R Code\nls() # List objects in the environment\n\n\n[1] \"x\" \"y\"\n\n\nShow R Code\nrm(x,y) # Remove objects\nls()\n\n\ncharacter(0)\n\n\nShow R Code\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Show R Code\n?matrix\n\n\nstarting httpd help server ... done\n\n\nShow R Code\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nShow R Code\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nShow R Code\nsqrt(x) # What does x look like?\n\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\n\nShow R Code\nx\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nShow R Code\nx^2\n\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nShow R Code\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n\n[1] 0.9930729\n\n\nShow R Code\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nShow R Code\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Show R Code\nmean(y)\n\n\n[1] 0.01103557\n\n\nShow R Code\nvar(y)\n\n\n[1] 0.7328675\n\n\nShow R Code\nsqrt(var(y))\n\n\n[1] 0.8560768\n\n\nShow R Code\nsd(y)\n\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "Show R Code\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\n\n\nShow R Code\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\n\n\nShow R Code\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\n\n\nShow R Code\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\n\npng \n  2 \n\n\nShow R Code\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nShow R Code\nx=1:10\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nShow R Code\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Niha Yadav",
    "section": "",
    "text": "I am a masters’ student studying social data analytics and research at The University of Texas at Dallas.\nContact me:\nniha.yadav@utdallas.edu Website"
  },
  {
    "objectID": "assignment7-im.html",
    "href": "assignment7-im.html",
    "title": "Assignment 7 -im",
    "section": "",
    "text": "Assigment 7 Labs 1 and 2\nInstructor Salaries App \nGeyser Data App"
  },
  {
    "objectID": "assignment5-km.html",
    "href": "assignment5-km.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Show R Code\n## NLP 1: text classification\n## Purpose: NLP workflow for text classification and prediction\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow R Code\nlibrary(tidymodels)\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\n\nShow R Code\nlibrary(stopwords)\nlibrary(textrecipes)\nlibrary(workflows)\n\n# Data Ingestion and Preparation\n# Read the CSV file\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus.csv\")\n\n\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow R Code\n# Inspect the first few rows\nhead(data)\n\n\n# A tibble: 6 × 2\n  text                                                       label    \n  &lt;chr&gt;                                                      &lt;chr&gt;    \n1 Many museums around the world display ancient artifacts.   Culture  \n2 Folk dances reflect the rich heritage of a region.         Culture  \n3 A documentary on classical music premiered last night.     Culture  \n4 Cultural festivals often bring communities together.       Culture  \n5 The gallery showcased local artists' paintings.            Culture  \n6 University scholarships can encourage academic excellence. Education\n\n\nShow R Code\ndata &lt;- data %&gt;%\n  mutate(label = factor(label)) # For classification\nset.seed(123)  # For reproducibility\n\n# Preparing training and test data\nsplit &lt;- initial_split(data, prop = 0.8, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# Text preprocessing\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                     # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;% # Keep top 100 tokens\n  step_tfidf(text)                             # Convert to TF-IDF\n\n# Model Specification and Training\nrf_spec &lt;- rand_forest(trees = 100) %&gt;% # More trees can lead to a more stable model\n  set_engine(\"ranger\") %&gt;% # ranger is a fast implementation of random forests\n  set_mode(\"classification\") # Good for high-dimensional feature space (e.g., TF-IDF vectors)\n\n# Preparing workflow combining preprocessing recipe and model specification.\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Model Evaluation and Prediction\n# Train the model on the training set\nrf_fit &lt;- wf %&gt;%\n  workflows::fit(data = train_data)\n\nrf_preds &lt;- predict(rf_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate performance\nmetrics(rf_preds, truth = label, estimate = .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass       0.1\n2 kap      multiclass       0  \n\n\nShow R Code\n# Confusion matrix\nconf_mat(rf_preds, truth = label, estimate = .pred_class)\n\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             0         0             0           0       0      0\n  Education           0         0             0           0       0      0\n  Entertainment       0         0             0           0       0      1\n  Environment         0         1             0           0       0      0\n  Finance             0         0             0           0       0      0\n  Health              1         0             1           1       0      0\n  Politics            0         0             0           0       1      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            0      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      1          0      0\n  Finance              0      0          0      0\n  Health               1      0          1      0\n  Politics             0      0          0      0\n  Sports               0      0          0      0\n  Technology           0      0          0      0\n  Travel               0      0          0      1\n\n\nShow R Code\n# Scale the workflow\n# Try on bigger dataset (200 cases)\n\ndata200 &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\")\n\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow R Code\ndata &lt;- data200 %&gt;%\n  mutate(label = factor(label))\nset.seed(123)  # For reproducibility\nsplit &lt;- initial_split(data, prop = 0.8, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%                      # Tokenize the text\n  step_stopwords(text) %&gt;%                    # Remove stopwords\n  step_tokenfilter(text, max_tokens = 100) %&gt;% # Keep top 100 tokens\n  step_tfidf(text)                             # Convert to TF-IDF\n\nrf_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_spec)\n\n# Train the model on the training set\nrf_fit &lt;- wf %&gt;%\n  workflows::fit(data = train_data)\n\n\nrf_preds &lt;- predict(rf_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate performance\nmetrics(rf_preds, truth = label, estimate = .pred_class)\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.875\n2 kap      multiclass     0.861\n\n\nShow R Code\n# Confusion matrix\nconf_mat(rf_preds, truth = label, estimate = .pred_class)\n\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             4         0             0           0       0      0\n  Education           0         4             0           0       0      0\n  Entertainment       0         0             4           0       0      0\n  Environment         0         0             0           2       0      0\n  Finance             0         0             0           0       3      0\n  Health              0         0             0           2       1      4\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            0      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               2      0          0      0\n  Politics             2      0          0      0\n  Sports               0      4          0      0\n  Technology           0      0          4      0\n  Travel               0      0          0      4"
  },
  {
    "objectID": "assignment4-km.html",
    "href": "assignment4-km.html",
    "title": "Assignment 4-km",
    "section": "",
    "text": "Show R Code\ninstall.packages(c(\"quanteda\",\"quanteda.textmodels\",\"quanteda.textstats\",\"quanteda.textplots\"))\n\n\nThe following package(s) will be installed:\n- quanteda            [4.3.1]\n- quanteda.textmodels [0.9.10]\n- quanteda.textplots  [0.95]\n- quanteda.textstats  [0.97.2]\nThese packages will be installed into \"~/Grad School/nihay17.github.io/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing quanteda ...                       OK [linked from cache]\n- Installing quanteda.textmodels ...            OK [linked from cache]\n- Installing quanteda.textstats ...             OK [linked from cache]\n- Installing quanteda.textplots ...             OK [linked from cache]\nSuccessfully installed 4 packages in 0.28 seconds.\n\n\nShow R Code\nlibrary(quanteda)\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nShow R Code\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow R Code\nhead(summit)\n\n\n# A tibble: 6 × 90\n  user_id status_id created_at          screen_name    text               source\n    &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt; \n1 1.38e18   1.46e18 2021-11-16 20:10:23 DSJ78992721    \"Breaking News: U… Twitt…\n2 2.60e 8   1.46e18 2021-11-16 20:10:17 bradhooperarch \"https://t.co/rKR… Twitt…\n3 3.00e 9   1.46e18 2021-11-16 20:10:10 scarecrow1113  \"[Recap] Biden ur… Twitt…\n4 3.00e 9   1.46e18 2021-11-15 19:24:04 scarecrow1113  \"U.S. President J… Twitt…\n5 1.36e18   1.46e18 2021-11-16 06:22:29 Internl_Leaks  \"#BREAKING Biden … Twitt…\n6 1.36e18   1.46e18 2021-11-16 20:09:36 Internl_Leaks  \"#BREAKING Biden … Twitt…\n# ℹ 84 more variables: display_text_width &lt;dbl&gt;, reply_to_status_id &lt;dbl&gt;,\n#   reply_to_user_id &lt;dbl&gt;, reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;,\n#   is_retweet &lt;lgl&gt;, favorite_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n#   quote_count &lt;lgl&gt;, reply_count &lt;lgl&gt;, hashtags &lt;chr&gt;, symbols &lt;chr&gt;,\n#   urls_url &lt;chr&gt;, urls_t.co &lt;chr&gt;, urls_expanded_url &lt;chr&gt;, media_url &lt;chr&gt;,\n#   media_t.co &lt;chr&gt;, media_expanded_url &lt;chr&gt;, media_type &lt;chr&gt;,\n#   ext_media_url &lt;chr&gt;, ext_media_t.co &lt;chr&gt;, ext_media_expanded_url &lt;chr&gt;, …\n\n\nShow R Code\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           159930 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\n\nShow R Code\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\n\nShow R Code\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\n\nShow R Code\nlibrary(\"quanteda.textplots\")\n\n# Network plot: tags\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\n\nShow R Code\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n\n\nShow R Code\n# Network plot: Users\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\n\nShow R Code\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_nfeat ... 10 more features, reached max_nfeat ... 701 more features ]\n\n\nShow R Code\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\n\n\n\n\n\nShow R Code\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;% \n  dfm() %&gt;% \n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\n\n\nShow R Code\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Biden\",\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npresident could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nevery could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncitizens could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nadministration could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprotected could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncountries could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nimmediately could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npanama could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsuccess could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nlonger could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndream could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nviolence could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\neveryone could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndefend could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsacred could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nend could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnational could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nconstitution could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npeople could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\neven could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nworld could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nway could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmoney could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nresolve could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchallenge could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nface could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nspent could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nborders could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nhear could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nshow could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntoday could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfamilies could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsystem could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndestiny could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntrue could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngreater could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngave could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchildren could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npeace could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\njustice could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprinciples could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntogether could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nvice could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncare could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nvote could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfear could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprosperity could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwomen could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nworkers could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nacross could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfuture could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngreatest could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nhonor could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nforward could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nserve could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwhether could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nbelieve could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npromise could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nearth could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npath could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nunderstand could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nbelongs could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncrisis could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmeet could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfather could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nafford could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nlove could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nhome could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchosen could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncongress could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndetermined could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nindependence could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nago could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndemands could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndignity could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nexample could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsupport could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncalled could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwant could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndefense could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nheart could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nyet could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nstronger could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnext could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nhappen could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncivil could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndeep could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncreated could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmake could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchange could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndifficult could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npolitical could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nenergy could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nbegin could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npride could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsomething could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\naccept could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nplanet could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncarry could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nknowledge could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nseek could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nadvance could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nraging could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npolitics could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntested could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndefeat could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npowerful could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nleaders could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npower could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\njobs could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nlight could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngenerations could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprogress could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nbroken could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsacrifice could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmeans could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nconfidence could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nopportunity could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmade could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfaith could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmeasure could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npatriots could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsafe could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nbirth could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nenemies could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nstrength could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsecurity could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfight could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nturn could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnation's could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchief could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npledge could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwashington could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchild could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndone could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsingle could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndifferences could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncitizen could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nquiet could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ninstitutions could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwithout could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nclinton could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncommunities could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npurpose could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsafety could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsoldiers could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndepends could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nspeaker could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmaking could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nborn could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nkeep could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nduties could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nremember could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nnothing could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ngather could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nsmall could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthroughout could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ninterests could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nlead could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntake could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nbegan could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmight could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntolerance could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nstorm could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ntaken could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchallenges could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprotect could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npast could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nalong could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npoverty could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nalliances could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ndefine could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nuse could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfathers could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\npeaceful could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmothers could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\ncall could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthink could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nmiddle could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nalways could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nfollow could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nresponsibility could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nwatching could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nyes could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nprosperous could not be fit on page. It will not be plotted.\n\n\nShow R Code\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\n\n\nShow R Code\n# Locate keywords-in-context\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n\n\nShow R Code\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"trade\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n\n\nShow R Code\ntokens_inaugural &lt;- tokens(data_corpus_inaugural_subset)\ntextplot_xray(\n  kwic(tokens_inaugural, pattern = \"american\"),\n  kwic(tokens_inaugural, pattern = \"people\"),\n  kwic(tokens_inaugural, pattern = \"trade\")\n)\n\n\n\n\n\n\n\n\n\n\nWordfish is a quantitative text analysis method used in political science and social sciences to estimate the ideological positions of texts or speakers based on word frequencies.\nWordfish is unsupervised while the scaling methods are supervised. Scaling methods require text references.\n\n\n\n\nShow R Code\n## Scraping 118th Congress - Committee on Foreign Affairs\n\n# Install required packages if not already installed\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n\nLoading required package: pacman\n\n\nShow R Code\npacman::p_load(purrr, magrittr, rjson, jsonlite, data.table, readr, dplyr)\n\n# Set working directory to where your CSV file is stored\nsetwd(\"C:/Users/Niha/OneDrive/Documents/Grad School/nihay17.github.io/files\")\n\n# Load the CSV file\ngovfiles &lt;- read.csv(\"govinfo-search-results-2025-03-13T22_07_34.csv\", skip = 2, stringsAsFactors = FALSE)\n\n# Display column names to check correct field names\nprint(colnames(govfiles))\n\n\n [1] \"X2\"                                                                                                                                                                                                                                                                                                                                                       \n [2] \"Congressional.Hearings\"                                                                                                                                                                                                                                                                                                                                   \n [3] \"CHRG.118hhrg52236\"                                                                                                                                                                                                                                                                                                                                        \n [4] \"CHRG.118hhrg52236.1\"                                                                                                                                                                                                                                                                                                                                      \n [5] \"Full.Committee.Markup.of.H.R..1690.and.H.R..589.Day.I.and.Day.2\"                                                                                                                                                                                                                                                                                          \n [6] \"https...www.govinfo.gov.app.details.CHRG.118hhrg52236.CHRG.118hhrg52236\"                                                                                                                                                                                                                                                                                  \n [7] \"https...www.govinfo.gov.content.pkg.CHRG.118hhrg52236.pdf.CHRG.118hhrg52236.pdf\"                                                                                                                                                                                                                                                                          \n [8] \"https...www.govinfo.gov.content.pkg.CHRG.118hhrg52236.html.CHRG.118hhrg52236.htm\"                                                                                                                                                                                                                                                                         \n [9] \"X\"                                                                                                                                                                                                                                                                                                                                                        \n[10] \"X.1\"                                                                                                                                                                                                                                                                                                                                                      \n[11] \"X.2\"                                                                                                                                                                                                                                                                                                                                                      \n[12] \"X.CONGRESS.FIRST.SESSION.APRIL.26..2023..DAY.1.APRIL.28..2023..DAY.2.Serial.No..118.22.Printed.for.the.use.of.the.Committee.on.Foreign.Affairs..GRAPHIC.NOT.AVAILABLE.IN.TIFF.FORMAT..Available..http...www.foreignaffairs.house.gov...http....docs.house.gov..or.http...www.govinfo.gov.U.S..GOVERNMENT.PUBLISHING.OFFICE.52.236PDF.WASHINGTON...2024...\"\n[13] \"X.3\"                                                                                                                                                                                                                                                                                                                                                      \n[14] \"X2023.04.26\"                                                                                                                                                                                                                                                                                                                                              \n\n\nShow R Code\n# Check values in the likely column (Congressional Hearings)\nhead(govfiles$Congressional.Hearings, 10)\n\n\n [1] \"Congressional Hearings\" \"Congressional Hearings\" \"Congressional Hearings\"\n [4] \"Congressional Hearings\" \"Congressional Hearings\" \"Congressional Hearings\"\n [7] \"Congressional Hearings\" \"Congressional Hearings\" \"Congressional Hearings\"\n[10] \"Congressional Hearings\"\n\n\nShow R Code\n# Filter for 118th Congress - Committee on Foreign Affairs\ngovfiles_filtered &lt;- govfiles %&gt;%\n  filter(grepl(\"118th Congress\", Congressional.Hearings, ignore.case = TRUE) & \n         grepl(\"Committee on Foreign Affairs\", Congressional.Hearings, ignore.case = TRUE))\n\n# Check the number of rows after filtering\nprint(nrow(govfiles_filtered))  # Should be greater than 0 if successful\n\n\n[1] 1\n\n\nShow R Code\nhead(govfiles_filtered)  # Preview filtered data\n\n\n            X2\n1 Search Terms\n                                                 Congressional.Hearings\n1 118th Congress Congressional Hearings in Committee on Foreign Affairs\n  CHRG.118hhrg52236 CHRG.118hhrg52236.1\n1                                      \n  Full.Committee.Markup.of.H.R..1690.and.H.R..589.Day.I.and.Day.2\n1                                                                \n  https...www.govinfo.gov.app.details.CHRG.118hhrg52236.CHRG.118hhrg52236\n1                                                                        \n  https...www.govinfo.gov.content.pkg.CHRG.118hhrg52236.pdf.CHRG.118hhrg52236.pdf\n1                                                                                \n  https...www.govinfo.gov.content.pkg.CHRG.118hhrg52236.html.CHRG.118hhrg52236.htm\n1                                                                                 \n   X X.1 X.2\n1 NA  NA  NA\n  X.CONGRESS.FIRST.SESSION.APRIL.26..2023..DAY.1.APRIL.28..2023..DAY.2.Serial.No..118.22.Printed.for.the.use.of.the.Committee.on.Foreign.Affairs..GRAPHIC.NOT.AVAILABLE.IN.TIFF.FORMAT..Available..http...www.foreignaffairs.house.gov...http....docs.house. ...\n1                                                                                                                                                                                                                                                               \n  X.3 X2023.04.26\n1  NA            \n\n\nShow R Code\n# Extract necessary fields\ngovfiles_filtered$id &lt;- govfiles_filtered$CHRG.118hhrg52236  # Adjust column if needed\npdf_govfiles_url &lt;- govfiles_filtered$`https...www.govinfo.gov.content.pkg.CHRG.118hhrg52236.pdf.CHRG.118hhrg52236.pdf`\npdf_govfiles_id &lt;- govfiles_filtered$id\n\n# Create a directory to save the PDFs if it doesn't exist\nsave_dir &lt;- \"C:/Users/Niha/OneDrive/Documents/Grad School/nihay17.github.io/files/gov_pdfs/\"\nif (!dir.exists(save_dir)) dir.create(save_dir)\n\n# Function to download PDFs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\")  # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Random sleep to prevent request blocking\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download first five files for testing\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\n\nStarting downloads\n\n\nShow R Code\nresults &lt;- 1:5 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n\n\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL '': status\nwas 'URL using bad/illegal format or missing URL'\n\n\nShow R Code\nmessage(\"Finished downloads\")\n\n\nFinished downloads\n\n\nShow R Code\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\nprint(time.taken)\n\n\nTime difference of 0.06858993 secs\n\n\nShow R Code\n# Print results\nprint(results)\n\n\n[1] \"Failed to download: \"   \"Failed to download: NA\" \"Failed to download: NA\"\n[4] \"Failed to download: NA\" \"Failed to download: NA\""
  },
  {
    "objectID": "assignment3-km.html",
    "href": "assignment3-km.html",
    "title": "Assignment 3 - Knowledge Mining",
    "section": "",
    "text": "Prompt: “Conduct a 2,000-word structured systematic literature review on the applications of data mining and machine learning in social media domains. Include a methodology section, synthesize key findings, identify trends and gaps, and propose one testable hypothesis. Use an academic tone and emulate systematic review standards.”\nResearch Proposal\nProposal"
  },
  {
    "objectID": "assignment3-km.html#grok-3",
    "href": "assignment3-km.html#grok-3",
    "title": "Assignment 3 - Knowledge Mining",
    "section": "2.1 Grok-3",
    "text": "2.1 Grok-3\n\nStructure: Yes\n\nSynthesis: Yes\n\nTrends and Gaps: Yes\n\nHypothesis: Yes\n\nReferences: Not fully cited; mentions rationale for omission"
  },
  {
    "objectID": "assignment3-km.html#chatgpt",
    "href": "assignment3-km.html#chatgpt",
    "title": "Assignment 3 - Knowledge Mining",
    "section": "2.2 ChatGPT",
    "text": "2.2 ChatGPT\n\nStructure: Yes\n\nSynthesis: Yes\n\nTrends and Gaps: Yes\n\nHypothesis: Yes\n\nReferences: None provided\n\n\n2.2.1 Strengths and Weaknesses\nGrok-3 demonstrated a superior structure and a more detailed approach, particularly in its elaboration of trends and gaps. Its response was organized into clear subsections, enhancing readability and coherence. In contrast, ChatGPT covered multiple subtopics but lacked subsection divisions, resulting in a less structured document. Additionally, ChatGPT provided no citations, though it suggested potential sources and retrieval methods, whereas Grok-3 included partial references and explained their absence."
  },
  {
    "objectID": "assignment3-km.html#grok-3-1",
    "href": "assignment3-km.html#grok-3-1",
    "title": "Assignment 3 - Knowledge Mining",
    "section": "3.1 Grok-3",
    "text": "3.1 Grok-3\nThe revised prompt emphasized a data scientist’s perspective. Grok-3 broke down the 2,000-word count into distinct sections, improving specificity and allocation of content."
  },
  {
    "objectID": "assignment3-km.html#chatgpt-1",
    "href": "assignment3-km.html#chatgpt-1",
    "title": "Assignment 3 - Knowledge Mining",
    "section": "3.2 ChatGPT",
    "text": "3.2 ChatGPT\nChatGPT refined its methodology section for clarity, incorporated recent advancements in machine learning techniques, and expanded its discussion of trends and research gaps."
  },
  {
    "objectID": "assignment3-dv.html",
    "href": "assignment3-dv.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Assignment 3 - Mapping Census Data\n\n\nShow R Code\nlibrary(tidycensus)\nlibrary(tigris)\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nShow R Code\nlibrary(sf)\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\n\nShow R Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow R Code\nlibrary(ggplot2)\nlibrary(readr)\n\ncensus_api_key(\"3bf1373ff0342dba5c6802db780897d0af31db3d\", install = FALSE)\n\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`.\n\n\nShow R Code\nreadRenviron(\"~/.Renviron\")  # Reload environment\n\n# Get ACS data \nincome_tx &lt;- get_acs(\n  geography = \"county\",\n  state = \"TX\",\n  variables = \"B19013_001\",  # Median household income\n  year = 2023,\n  geometry = TRUE\n)\n\n\nGetting data from the 2019-2023 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nShow R Code\n# Plot the map\nggplot(income_tx, aes(fill = estimate)) +\n  geom_sf(color = \"white\", size = 0.1) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Median Income ($)\") +\n  labs(\n    title = \"Median Household Income by County in Texas (ACS 2023)\",\n    caption = \"Source: U.S. Census Bureau, ACS 5-Year Estimates\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow R Code\n# Get population data\npop_tx &lt;- get_acs(\n  geography = \"county\",\n  state = \"TX\",\n  variables = \"B01003_001\",  # Total population\n  year = 2023\n)\n\n\nGetting data from the 2019-2023 5-year ACS\n\n\nShow R Code\n# Top 10 counties\ntop10_pop &lt;- pop_tx %&gt;%\n  arrange(desc(estimate)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(NAME, estimate)\n\n# View table\nprint(top10_pop)\n\n\n# A tibble: 10 × 2\n   NAME                    estimate\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 Harris County, Texas     4758579\n 2 Dallas County, Texas     2603816\n 3 Tarrant County, Texas    2135743\n 4 Bexar County, Texas      2037344\n 5 Travis County, Texas     1307625\n 6 Collin County, Texas     1116601\n 7 Denton County, Texas      945644\n 8 Hidalgo County, Texas     880921\n 9 El Paso County, Texas     866275\n10 Fort Bend County, Texas   859721\n\n\nThe map shows the median income of counties in Texas, with darker colors indicating higher income levels. The table lists the top 10 most populous counties in Texas based on the 2023 ACS data."
  },
  {
    "objectID": "assignment2-im.html",
    "href": "assignment2-im.html",
    "title": "Assignment 2-IM",
    "section": "",
    "text": "A relation schema is the structure or blueprint of a table, defining column names and data types. An example would be: Students(student_id, name, major, age). A relation is the actual table itself, including column names and all stored records. An example would be a table named Students, which has all of the student data. The instance is the current snapshot of data in a table at a given time. An example would be a specific set of student records at a moment in time.\n\n\n\n\nShow R Code\nlibrary(DiagrammeR)\nlibrary(DiagrammeRsvg)\nlibrary(rsvg)\n\n\nLinking to librsvg 2.57.0\n\n\nShow R Code\n# Create the diagram and store it in a variable\ndiagram &lt;- grViz(\"\n  digraph bank_database {\n    graph [layout = dot, rankdir = LR]\n\n    node [shape = plaintext]\n\n    branch [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Branch&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;branch_name (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;branch_city&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;assets&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    customer [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Customer&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;ID (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;customer_name&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;customer_street&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;customer_city&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    loan [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Loan&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;loan_number (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;branch_name (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;amount&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    borrower [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Borrower&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;ID (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;loan_number (FK)&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    account [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Account&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;account_number (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;branch_name (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;balance&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    depositor [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Depositor&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;ID (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;account_number (FK)&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    # Define relationships\n    branch -&gt; loan [label = 'has loans']\n    branch -&gt; account [label = 'holds accounts']\n    customer -&gt; borrower [label = 'borrows']\n    customer -&gt; depositor [label = 'deposits']\n    loan -&gt; borrower [label = 'is borrowed']\n    account -&gt; depositor [label = 'is linked']\n  }\n\")\n\n# Convert DiagrammeR graph to SVG format\nsvg_code &lt;- export_svg(diagram)\n\n# Save as an SVG file\nwriteLines(svg_code, \"bank_schema.svg\")\n\n# Convert SVG to PNG\nrsvg_png(\"bank_schema.svg\", \"bank_schema.png\")\n\n\n\n\n\nBank Database Schema\n\n\n\nLoan - branch_name Borrower- ID & loan_number Account- branch_name Depositor- account_number & ID"
  },
  {
    "objectID": "assignment1-km.html",
    "href": "assignment1-km.html",
    "title": "Assignment 1 - Comparing Modeling Approaches",
    "section": "",
    "text": "The journal article “To Explain or to Predict?” by Galit Shmueli clarifies the distinction between explanatory modeling and predictive modeling, emphasizing their different goals and implications.\n\n\n\nExplanatory modeling aims to understand causal relationships.\n\nPredictive modeling focuses on forecasting future observations.\n\nHigh explanatory power ≠ High predictive power.\n\nResearchers should clearly define their modeling objective before choosing an approach."
  },
  {
    "objectID": "assignment1-km.html#shmueli",
    "href": "assignment1-km.html#shmueli",
    "title": "Assignment 1 - Comparing Modeling Approaches",
    "section": "",
    "text": "The journal article “To Explain or to Predict?” by Galit Shmueli clarifies the distinction between explanatory modeling and predictive modeling, emphasizing their different goals and implications.\n\n\n\nExplanatory modeling aims to understand causal relationships.\n\nPredictive modeling focuses on forecasting future observations.\n\nHigh explanatory power ≠ High predictive power.\n\nResearchers should clearly define their modeling objective before choosing an approach."
  },
  {
    "objectID": "assignment1-km.html#breiman",
    "href": "assignment1-km.html#breiman",
    "title": "Assignment 1 - Comparing Modeling Approaches",
    "section": "2 Statistical Modeling: The Two Cultures - Leo Breiman",
    "text": "2 Statistical Modeling: The Two Cultures - Leo Breiman\nThe article “Statistical Modeling: The Two Cultures” by Leo Breiman critiques the over-reliance on data modeling within traditional statistics and argues for greater use of algorithmic modeling (e.g., machine learning).\n\n2.1 Key Points:\n\nData modeling (parametric models, like regression) dominates traditional statistics.\n\nAlgorithmic modeling (machine learning, decision trees, random forests) is often better for prediction.\n\nWith the rise of large datasets, statisticians should broaden their methods.\n\nBreiman calls for a more flexible and diverse statistical toolbox."
  },
  {
    "objectID": "assignment1-km.html#comparison",
    "href": "assignment1-km.html#comparison",
    "title": "Assignment 1 - Comparing Modeling Approaches",
    "section": "3 Comparison & Conclusion",
    "text": "3 Comparison & Conclusion\nBoth papers highlight the importance of choosing the right modeling approach based on research goals.\n\n\n\n\n\n\n\n\nFeature\nShmueli (2010) - Explanation vs. Prediction\nBreiman (2001) - Two Cultures\n\n\n\n\nFocus\nDifferentiating explanatory & predictive modeling\nAdvocating for algorithmic modeling over parametric methods\n\n\nKey Argument\nModels should align with the research goal\nStatistics should embrace machine learning\n\n\nImpact\nGuides researchers in study design & model selection\nPushed for machine learning adoption in statistics\n\n\n\n\n3.1 Final Thoughts\n\nShmueli (2010) is crucial for researchers to decide between explanation vs. prediction.\n\nBreiman (2001) challenges traditional statistics to embrace algorithmic modeling.\n\nBoth papers shape modern discussions in data science and statistical modeling."
  },
  {
    "objectID": "assignment1-dv.html",
    "href": "assignment1-dv.html",
    "title": "Assignment 1- Data Visualization",
    "section": "",
    "text": "1 Assignment 1\nThis is my website create using Quarto. I decided to use the theme simplex. The navigation bar includes links to my assignments from previous classes and my resume."
  },
  {
    "objectID": "assignment1-im.html",
    "href": "assignment1-im.html",
    "title": "Assignment 1 - Information Management",
    "section": "",
    "text": "Question 1\n\nQuestion 2\n\nQuestion 3\n\nQuestion 6"
  },
  {
    "objectID": "assignment1-im.html#navigation",
    "href": "assignment1-im.html#navigation",
    "title": "Assignment 1 - Information Management",
    "section": "",
    "text": "Question 1\n\nQuestion 2\n\nQuestion 3\n\nQuestion 6"
  },
  {
    "objectID": "assignment1-im.html#question1",
    "href": "assignment1-im.html#question1",
    "title": "Assignment 1 - Information Management",
    "section": "2 Question 1: University Systems",
    "text": "2 Question 1: University Systems\n\n2.1 University System (Student Information System)\nPurpose: Manages and maintains student information.\nFunctions:\n- Course Registration\n- Academic Records\n- Personal Information\n- Financial Information\nExample: UT Dallas uses Oracle PeopleSoft Campus Solutions\n\n\n2.2 Online Learning Platform (Learning Management System)\nPurpose: Facilitates electronic learning and communication.\nFunctions:\n- Courses\n- Grades\n- Calendar\n- Communication\nExample: UT Dallas uses Blackboard’s e-learning\n\n\n2.3 Online Banking System (Financial System)\nPurpose: Offers financial management and access to banking systems.\nFunctions:\n- Money Transfer\n- Security\n- Mobile Banking\n- Account Management\nExample: Bank of America Online Banking"
  },
  {
    "objectID": "assignment1-im.html#question2",
    "href": "assignment1-im.html#question2",
    "title": "Assignment 1 - Information Management",
    "section": "3 Question 2: Data-Driven Platforms",
    "text": "3 Question 2: Data-Driven Platforms\n\n3.1 Criminology: Crime Data Analysis Platform\nPurpose: Analyze and visualize crime data, helping law enforcement agencies identify crime patterns and improve public safety.\nFunctions:\nData Visualization (heatmaps, trends).\nPredictive Analytics (future crime occurrences).\nReporting (crime statistics reports).\nCollaboration (data sharing, inter-agency tools).\nSimple Interface Design:\n- Dashboard: Key crime metrics, maps, trend charts.\n- Analysis Tools: Filtering data by crime type, location, and time.\n- Reports: Generate and export reports.\n- Collaboration: Chat and document sharing.\n\n\n\n3.2 Brain Science: Cognitive Training App\nPurpose: Provide cognitive training exercises and assessments to track cognitive health.\nFunctions:\nTraining Programs (memory, problem-solving).\nProgress Tracking (performance feedback).\nAssessments (evaluate brain function).\nReminders (daily training sessions).\nSimple Interface Design:\n- Home Screen: Dashboard with progress charts.\n- Training Exercises: Personalized cognitive tasks.\n- Assessments: Cognitive evaluations and reports.\n- Reminders: Notifications for training sessions.\n\n\n\n3.3 Economics: Personal Finance Management App\nPurpose: Help individuals manage personal finances, track expenses, and plan future goals.\nFunctions:\nBudgeting (set budgets, track spending).\nFinancial Goals (saving, debt payoff).\nExpense Categorization (spending patterns).\nReports (financial visualizations).\nSimple Interface Design:\n- Home Screen: Budget status, spending summary.\n- Expense Tracker: Add and categorize expenses.\n- Financial Goals: Set and monitor savings progress.\n- Reports: Charts to visualize financial health."
  },
  {
    "objectID": "assignment1-im.html#question3",
    "href": "assignment1-im.html#question3",
    "title": "Assignment 1 - Information Management",
    "section": "4 Question 3: Importance of Data Mining",
    "text": "4 Question 3: Importance of Data Mining\nWhy is data mining necessary?\n✔ Discovering hidden patterns in data.\n✔ Predicting future trends.\n✔ Improving decision-making.\n✔ Enhancing efficiency.\n✔ Personalizing user experiences."
  },
  {
    "objectID": "assignment1-im.html#question6",
    "href": "assignment1-im.html#question6",
    "title": "Assignment 1 - Information Management",
    "section": "5 Question 6: Example Tables (Users, Posts, Comments)",
    "text": "5 Question 6: Example Tables (Users, Posts, Comments)\nBelow are three database tables used in a typical social media system:\n\n\nShow R Code\n# Users table\nusers_table &lt;- data.frame(\n  user_id = 1:5,\n  username = c(\"john_doe\", \"alice_wonder\", \"bob_smith\", \"carol_jones\", \"eve_wood\"),\n  email = c(\"john@example.com\", \"alice@example.com\", \"bob@example.com\", \"carol@example.com\", \"eve@example.com\"),\n  created_at = Sys.time() - sample(1:1000, 5),\n  updated_at = Sys.time()\n)\n# Display the table\nusers_table\n\n\n  user_id     username             email          created_at\n1       1     john_doe  john@example.com 2025-09-02 17:08:28\n2       2 alice_wonder alice@example.com 2025-09-02 17:09:18\n3       3    bob_smith   bob@example.com 2025-09-02 17:19:08\n4       4  carol_jones carol@example.com 2025-09-02 17:08:50\n5       5     eve_wood   eve@example.com 2025-09-02 17:16:25\n           updated_at\n1 2025-09-02 17:19:09\n2 2025-09-02 17:19:09\n3 2025-09-02 17:19:09\n4 2025-09-02 17:19:09\n5 2025-09-02 17:19:09\n\n\n\n\nShow R Code\n# Posts table\nposts_table &lt;- data.frame(\n  post_id = 1:5,\n  user_id = c(1, 2, 1, 3, 4),\n  content = c(\"First post\", \"Second post\", \"Another post\", \"Interesting post\", \"Final post\"),\n  created_at = Sys.time() - sample(1:1000, 5),\n  updated_at = Sys.time(),\n  likes_count = c(10, 5, 8, 2, 7),\n  comments_count = c(2, 1, 4, 0, 3),\n  shared_count = c(3, 1, 2, 0, 1)\n)\n# Display the table\nposts_table\n\n\n  post_id user_id          content          created_at          updated_at\n1       1       1       First post 2025-09-02 17:17:23 2025-09-02 17:19:09\n2       2       2      Second post 2025-09-02 17:11:45 2025-09-02 17:19:09\n3       3       1     Another post 2025-09-02 17:14:37 2025-09-02 17:19:09\n4       4       3 Interesting post 2025-09-02 17:16:36 2025-09-02 17:19:09\n5       5       4       Final post 2025-09-02 17:07:57 2025-09-02 17:19:09\n  likes_count comments_count shared_count\n1          10              2            3\n2           5              1            1\n3           8              4            2\n4           2              0            0\n5           7              3            1\n\n\n\n\nShow R Code\n# Comments table\ncomments_table &lt;- data.frame(\n  comment_id = 1:5,\n  post_id = c(1, 1, 2, 3, 4),\n  user_id = c(2, 3, 1, 4, 5),\n  content = c(\"Great post!\", \"Interesting!\", \"Thanks for sharing!\", \"Nice!\", \"I agree!\"),\n  created_at = Sys.time() - sample(1:1000, 5),\n  updated_at = Sys.time(),\n  likes_count = c(5, 2, 3, 1, 4)\n)\n# Display the table\ncomments_table\n\n\n  comment_id post_id user_id             content          created_at\n1          1       1       2         Great post! 2025-09-02 17:06:50\n2          2       1       3        Interesting! 2025-09-02 17:08:46\n3          3       2       1 Thanks for sharing! 2025-09-02 17:16:15\n4          4       3       4               Nice! 2025-09-02 17:17:46\n5          5       4       5            I agree! 2025-09-02 17:13:33\n           updated_at likes_count\n1 2025-09-02 17:19:10           5\n2 2025-09-02 17:19:10           2\n3 2025-09-02 17:19:10           3\n4 2025-09-02 17:19:10           1\n5 2025-09-02 17:19:10           4"
  },
  {
    "objectID": "assignment2-dv.html",
    "href": "assignment2-dv.html",
    "title": "Assignment 2-Data Visualization",
    "section": "",
    "text": "Assignment 2\nGoogle Trends Data\n\n\nShow R Code\n## EPPS 6302 Methods of Data Collection and Production\n## Google Trends with R\n\ninstall.packages(\"gtrendsR\")\n\n\nrenv was unable to query available packages from the following repositories:\n- # http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4 -----------------\nerror downloading 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES.rds' [error code 22]\nerror downloading 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES.gz' [error code 22]\nerror downloading 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.4/PACKAGES' [error code 22]\n\n\nThe following package(s) will be installed:\n- gtrendsR [1.5.2]\nThese packages will be installed into \"~/Grad School/nihay17.github.io/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing gtrendsR ...                       OK [linked from cache]\nSuccessfully installed 1 package in 43 milliseconds.\n\n\nShow R Code\nlibrary(gtrendsR)\n\n# First query\nTrumpHarrisElection &lt;- gtrends(c(\"Trump\", \"Harris\", \"election\"),\n                               onlyInterest = TRUE,\n                               geo = \"US\",\n                               gprop = \"web\",\n                               time = \"today 12-m\")  # Shorter time range\nSys.sleep(10)\n\nthe_df &lt;- TrumpHarrisElection$interest_over_time\nplot(TrumpHarrisElection)\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the gtrendsR package.\n  Please report the issue at &lt;https://github.com/PMassicotte/gtrendsR/issues&gt;.\n\n\n\n\n\n\n\n\n\nShow R Code\n# Second query\ntg &lt;- gtrends(\"tariff\", time = \"today 12-m\")\nSys.sleep(10)\n\n# Third query\nplot(gtrends(\"tariff\", time = \"today 12-m\"))\n\n\n\n\n\n\n\n\n\nShow R Code\nSys.sleep(10)\n\n# Fourth query\nplot(gtrends(\"tariff\", geo = \"GB\", time = \"today 12-m\"))\n\n\n\n\n\n\n\n\n\nShow R Code\nSys.sleep(10)\n\n# Fifth query\nplot(gtrends(\"tariff\", geo = c(\"US\", \"GB\", \"TW\"), time = \"today 12-m\"))\n\n\n\n\n\n\n\n\n\nShow R Code\nSys.sleep(10)\n\n# Sixth query\ntg_iot &lt;- tg$interest_over_time\ntct &lt;- gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"today 12-m\")\nSys.sleep(10)\n\ntct &lt;- data.frame(tct$interest_over_time)\nplot(gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"today 12-m\"))"
  },
  {
    "objectID": "assignment2-km.html",
    "href": "assignment2-km.html",
    "title": "Assignment2-Knowledge Mining",
    "section": "",
    "text": "Labs for this Assignment\n-Lab 1\n-Lab 2\n3-7\n\n\nShow R Code\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Prepare the analyze the Party ID variable \n# Assign label to the values (1=KMT, 2=DPP, 3=NP, 4=PFP, 5=TSU, 6=NPP, 7=\"NA\")\n\nTEDS_2016$PartyID &lt;- factor(TEDS_2016$PartyID, labels=c(\"KMT\",\"DPP\",\"NP\",\"PFP\", \"TSU\", \"NPP\",\"NA\"))\n\n\n\n\nShow R Code\nattach(TEDS_2016)\nhead(PartyID)\n\n\n[1] NA  NA  KMT NA  NA  DPP\nLevels: KMT DPP NP PFP TSU NPP NA\n\n\nShow R Code\ntail(PartyID)\n\n\n[1] NA  NA  DPP NA  NA  NA \nLevels: KMT DPP NP PFP TSU NPP NA\n\n\n\n\nShow R Code\nlibrary(descr)\nfreq(TEDS_2016$PartyID)\n\n\n\n\n\n\n\n\n\nTEDS_2016$PartyID \n      Frequency  Percent\nKMT         388  22.9586\nDPP         591  34.9704\nNP            3   0.1775\nPFP          32   1.8935\nTSU           5   0.2959\nNPP          43   2.5444\nNA          628  37.1598\nTotal      1690 100.0000\n\n\n\n\nShow R Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow R Code\nTEDS_2016 %&gt;% \n  count(PartyID) %&gt;% \n  mutate(perc = n / nrow(TEDS_2016)) -&gt; T2\nggplot(T2, aes(x = reorder(PartyID, -perc),y = perc,fill=PartyID)) + \n  geom_bar(stat = \"identity\") +\n  ylab(\"Party Support (%)\") + \n  xlab(\"Taiwan Political Parties\") +\n  theme_bw() +\n  scale_fill_manual(values=c(\"steel blue\",\"forestgreen\",\"khaki1\",\"orange\",\"goldenrod\",\"yellow\",\"grey\"))\n\n\n\n\n\n\n\n\n\n\n\nShow R Code\nTEDS_2016$Tondu&lt;-factor(TEDS_2016$Tondu,labels=c(\"Unification now\",\"Status quo, unif. in future\",\"Status quo, decide later\",\"Status quo forever\", \"Status quo, indep. in future\", \"Independence now\",\"No response\"))\n\n\nfreq_table &lt;- table(TEDS_2016$Tondu)\nprint(freq_table)  # Display the frequency table\n\n\n\n             Unification now  Status quo, unif. in future \n                          27                          180 \n    Status quo, decide later           Status quo forever \n                         546                          328 \nStatus quo, indep. in future             Independence now \n                         380                          108 \n                 No response \n                         121 \n\n\nShow R Code\nlibrary(ggplot2)\nggplot(data = as.data.frame(freq_table), aes(x = Var1, y = Freq)) +\n  geom_bar(stat=\"identity\", fill=\"steelblue\") +\n  labs(title=\"Public Opinion on Taiwan's Status (TEDS 2016)\", \n       x=\"Opinion\", y=\"Frequency\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate labels for readability"
  },
  {
    "objectID": "assignment3-im.html",
    "href": "assignment3-im.html",
    "title": "assignment3-im",
    "section": "",
    "text": "Show R Code\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Connect to the SQLite database file\ncon &lt;- dbConnect(RSQLite::SQLite(), \"files/sql.db\")\n\n\n\n\n\n\n\nShow R Code\nSELECT DISTINCT ID FROM takes;\n\n\n\nDisplaying records 1 - 10\n\n\nID\n\n\n\n\n00128\n\n\n12345\n\n\n19991\n\n\n23121\n\n\n44553\n\n\n45678\n\n\n54321\n\n\n55739\n\n\n76543\n\n\n76653\n\n\n\n\n\n\n\nShow R Code\nSELECT ID, name, dept_name, salary FROM instructor;\n\n\n\nDisplaying records 1 - 10\n\n\nID\nname\ndept_name\nsalary\n\n\n\n\n10101\nSrinivasan\nComp. Sci.\n65000\n\n\n12121\nWu\nFinance\n90000\n\n\n15151\nMozart\nMusic\n40000\n\n\n22222\nEinstein\nPhysics\n95000\n\n\n32343\nEl Said\nHistory\n60000\n\n\n33456\nGold\nPhysics\n87000\n\n\n45565\nKatz\nComp. Sci.\n75000\n\n\n58583\nCalifieri\nHistory\n62000\n\n\n76543\nSingh\nFinance\n80000\n\n\n76766\nCrick\nBiology\n72000\n\n\n\n\n\n\n\nShow R Code\nSELECT DISTINCT dept_name FROM department;\n\n\n\n7 records\n\n\ndept_name\n\n\n\n\nBiology\n\n\nComp. Sci.\n\n\nElec. Eng.\n\n\nFinance\n\n\nHistory\n\n\nMusic\n\n\nPhysics\n\n\n\n\n\n\n\n\n\n\nShow R Code\nSELECT DISTINCT student.ID, student.name\nFROM student\nJOIN takes ON student.ID = takes.ID\nJOIN course ON takes.course_id = course.course_id\nJOIN department ON course.dept_name = department.dept_name\nWHERE department.dept_name = 'Comp. Sci.';\n\n\n\n6 records\n\n\nID\nname\n\n\n\n\n00128\nZhang\n\n\n12345\nShankar\n\n\n45678\nLevy\n\n\n54321\nWilliams\n\n\n76543\nBrown\n\n\n98765\nBourikas\n\n\n\n\n\n\n\nShow R Code\nSELECT DISTINCT student.ID, student.name, takes.grade\nFROM student\nJOIN takes ON student.ID = takes.ID\nJOIN course ON takes.course_id = course.course_id\nJOIN department ON course.dept_name = department.dept_name\nWHERE department.dept_name = 'Comp. Sci.';\n\n\n\nDisplaying records 1 - 10\n\n\nID\nname\ngrade\n\n\n\n\n00128\nZhang\nA\n\n\n00128\nZhang\nA-\n\n\n12345\nShankar\nC\n\n\n12345\nShankar\nA\n\n\n45678\nLevy\nF\n\n\n45678\nLevy\nB+\n\n\n45678\nLevy\nB\n\n\n54321\nWilliams\nA-\n\n\n54321\nWilliams\nB+\n\n\n76543\nBrown\nA\n\n\n\n\n\n\n\nShow R Code\nSELECT DISTINCT s.ID, s.name\nFROM student s\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM takes t\n    JOIN section sec ON t.course_id = sec.course_id AND t.sec_id = sec.sec_id AND t.semester = sec.semester AND t.year = sec.year\n    WHERE t.ID = s.ID AND sec.year &lt; 2017\n);\n\n\n\nDisplaying records 1 - 10\n\n\nID\nname\n\n\n\n\n00128\nZhang\n\n\n12345\nShankar\n\n\n19991\nBrandt\n\n\n23121\nChavez\n\n\n44553\nPeltier\n\n\n45678\nLevy\n\n\n54321\nWilliams\n\n\n55739\nSanchez\n\n\n70557\nSnow\n\n\n76543\nBrown\n\n\n\n\n\n\n\nShow R Code\nSELECT dept_name, MAX(salary) AS max_salary\nFROM instructor\nGROUP BY dept_name;\n\n\n\n7 records\n\n\ndept_name\nmax_salary\n\n\n\n\nBiology\n72000\n\n\nComp. Sci.\n92000\n\n\nElec. Eng.\n80000\n\n\nFinance\n90000\n\n\nHistory\n62000\n\n\nMusic\n40000\n\n\nPhysics\n95000\n\n\n\n\n\n\n\nShow R Code\nSELECT MIN(max_salary) AS lowest_max_salary\nFROM (\n    SELECT dept_name, MAX(salary) AS max_salary\n    FROM instructor\n    GROUP BY dept_name\n) AS dept_max_salaries;\n\n\n\n1 records\n\n\nlowest_max_salary\n\n\n\n\n40000\n\n\n\n\n\n\n\nShow R Code\nSELECT i.dept_name, i.ID, i.name, i.salary\nFROM instructor i\nJOIN (\n    SELECT dept_name, MAX(salary) AS max_salary\n    FROM instructor\n    GROUP BY dept_name\n) AS max_salaries\nON i.dept_name = max_salaries.dept_name AND i.salary = max_salaries.max_salary\nORDER BY max_salaries.max_salary;\n\n\n\n7 records\n\n\ndept_name\nID\nname\nsalary\n\n\n\n\nMusic\n15151\nMozart\n40000\n\n\nHistory\n58583\nCalifieri\n62000\n\n\nBiology\n76766\nCrick\n72000\n\n\nElec. Eng.\n98345\nKim\n80000\n\n\nFinance\n12121\nWu\n90000\n\n\nComp. Sci.\n83821\nBrandt\n92000\n\n\nPhysics\n22222\nEinstein\n95000\n\n\n\n\n\n\n\n\n\n\nShow R Code\nSELECT i.ID, i.name\nFROM instructor i\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM teaches t\n    JOIN takes tk ON t.course_id = tk.course_id AND t.sec_id = tk.sec_id AND t.semester = tk.semester AND t.year = tk.year\n    WHERE t.ID = i.ID AND tk.grade = 'A'\n);\n\n\n\n9 records\n\n\nID\nname\n\n\n\n\n12121\nWu\n\n\n15151\nMozart\n\n\n22222\nEinstein\n\n\n32343\nEl Said\n\n\n33456\nGold\n\n\n45565\nKatz\n\n\n58583\nCalifieri\n\n\n76543\nSingh\n\n\n98345\nKim\n\n\n\n\n\n\n\nShow R Code\ndbDisconnect(con)"
  },
  {
    "objectID": "assignment4-im.html",
    "href": "assignment4-im.html",
    "title": "Assignment 4",
    "section": "",
    "text": "A strong entity has a primary key that uniquely identifies each record. It does not depend on any other entity for its existence.\nExample: Buildings in a City\nBuildings can be uniquely identified by their building_id. They do not require another entity to exist.\nA weak entity cannot exist without a strong entity. It does not have a sufficient primary key on its own and instead uses a foreign key + a discriminator (partial key) to form a composite primary key.\nExample: Apartments within a Building\nApartments exist inside buildings and are dependent on a Building. An apartment number (apt_no) alone is not unique across all buildings. To uniquely identify an apartment, we need both building_id and apt_no.\n\n\n\n\n\nShow R Code\nlibrary(DiagrammeR)\nlibrary(DiagrammeRsvg)\nlibrary(rsvg)\n\n\nLinking to librsvg 2.57.0\n\n\nShow R Code\n# Create the diagram and store it in a variable\ndiagram &lt;- grViz(\"\n  digraph league_database {\n    graph [layout = dot, rankdir = LR]\n\n    node [shape = plaintext]\n\n    League [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;League&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;league_id (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;name&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    Team [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Team&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;team_id (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;name&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;league_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    Match [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Match&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;match_id (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;date&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;location&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    Match_Teams [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Match_Teams&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;match_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;team_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;team_score&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    Player [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Player&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;&lt;U&gt;player_id (PK)&lt;/U&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;name&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;position&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    PlayerHistory [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;PlayerHistory&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;player_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;team_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;start_date&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;end_date&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    Plays_In [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;Plays_In&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;match_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;player_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;minutes_played&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    PlayerStats [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;PlayerStats&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;match_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;player_id (FK)&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;points&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;assists&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;rebounds&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;fouls&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;]\n\n    SummaryStats [label = &lt;\n      &lt;TABLE BORDER='1' CELLBORDER='1' CELLSPACING='0'&gt;\n        &lt;TR&gt;&lt;TD COLSPAN='2'&gt;&lt;B&gt;SummaryStats&lt;/B&gt;&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;total_points&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;avg_points&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;total_assists&lt;/TD&gt;&lt;/TR&gt;\n        &lt;TR&gt;&lt;TD&gt;avg_assists&lt;/TD&gt;&lt;/TR&gt;\n      &lt;/TABLE&gt;&gt;, shape=ellipse];\n\n    # Define relationships\n    League -&gt; Team [label='contains']\n    Team -&gt; PlayerHistory [label='has players']\n    Match -&gt; Match_Teams [label='includes']\n    Match_Teams -&gt; Team [label='participating']\n    Match -&gt; Plays_In [label='includes']\n    Player -&gt; PlayerHistory [label='belongs to']\n    Player -&gt; Plays_In [label='participates']\n    Plays_In -&gt; PlayerStats [label='records']\n    PlayerStats -&gt; SummaryStats [label='aggregates']\n  }\n\")\n\n# Convert DiagrammeR graph to SVG format\nsvg_code &lt;- export_svg(diagram)\n\n# Save as an SVG file\nwriteLines(svg_code, \"league_schema.svg\")\n\n# Convert SVG to PNG\nrsvg_png(\"league_schema.svg\", \"league_schema.png\")\n\n\n\n\n\nLeague Database Schema\n\n\n\n\n\n\n\n\n\nThe takes table already has the columns used for the group by and where clause. So it already has the information and does not need to join another table. ii.\n\n\nShow R Code\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Connect to the SQLite database file\ncon &lt;- dbConnect(RSQLite::SQLite(), \"files/sql.db\")\n\n\n\n\nShow R Code\nselect course_id, semester, year, sec_id, avg (tot_cred) \nfrom takes natural join student \nwhere year = 2017 \ngroup by course_id, semester, year, sec_id \nhaving count (ID) &gt;=2;\n\n\n\n3 records\n\n\ncourse_id\nsemester\nyear\nsec_id\navg (tot_cred)\n\n\n\n\nCS-101\nFall\n2017\n1\n65\n\n\nCS-190\nSpring\n2017\n2\n43\n\n\nCS-347\nFall\n2017\n1\n67\n\n\n\n\n\n\n\n\n\n\nShow R Code\nSELECT s.ID\nFROM student s\nLEFT JOIN takes t ON s.ID = t.ID\nWHERE t.ID IS NULL;\n\n\n\n1 records\n\n\nID\n\n\n\n\n70557\n\n\n\n\n\n\n\nShow R Code\ndbDisconnect(con)\n\n\nResearch Proposal\nProposal"
  },
  {
    "objectID": "assignment5-im.html",
    "href": "assignment5-im.html",
    "title": "Assignment 5-im",
    "section": "",
    "text": "A disconnected graph means that some entities are not related to the main schema.\nA graph with a cycle means that some entities form a loop in relationships.\n\n\n\n\n\nWhile we could convert weak entity sets into strong ones, it’s not always the best choice because weak entities better represent real-world relationships and help maintain database integrity efficiently. It helps avoid data redundancy, allow natural composite keys, and they improve efficiency by saveing storage space and speeding up queries.\n4a.\n\n\n\nSELECT e.ID, e.person_name FROM employee e JOIN works w ON e.ID = w.ID JOIN company c ON w.company_name = c.company_name WHERE e.city = c.city;\n\n\n\nSELECT e.ID, e.person_name FROM employee e JOIN manages m ON e.ID = m.ID JOIN employee mgr ON m.manager_id = mgr.ID WHERE e.city = mgr.city AND e.street = mgr.street;\n\n\n\nSELECT w.ID, e.person_name FROM works w JOIN employee e ON w.ID = e.ID WHERE w.salary &gt; ( SELECT AVG(w2.salary) FROM works w2 WHERE w2.company_name = w.company_name );\n4b.\nThe problem is that NATURAL JOIN is used across multiple tables. This can lead to different results if the tables already share the same attributes."
  },
  {
    "objectID": "assignment6-im.html",
    "href": "assignment6-im.html",
    "title": "Assignment 6-im",
    "section": "",
    "text": "Stack Exchange API Technologies used:\nOperating System: Microsoft Windows Server 2019 x64\nWeb Server: IIS 10\nDatabase: SQL Server 2019 running on Microsoft Windows Server 2016 x64\nProgramming Language: C#\nSoftware Development Tools: Visual Studio 2019\nFramework: Microsoft .NET 6.0 and ASP.NET Core 6.0\nView Engine: Razor\nBrowser Framework: jQuery 1.12.4\nData Access Layer: Entity Framework Core 2.2 and Dapper\nCache / Additional Data: Redis 4.0.7 via StackExchange.Redis, with serialization via protobuf-net\nSource Control: Git using a GitHub Enterprise instance hosted by GitHub\n\n\n\n\n\n\n\n\n\nShow R Code\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Connect to the SQLite database file\ncon &lt;- dbConnect(RSQLite::SQLite(), \"files/sql.db\")\n\n\n\n\nShow R Code\nSELECT student.ID\nFROM student\nLEFT OUTER JOIN advisor ON student.ID = advisor.s_id\nWHERE advisor.s_id IS NULL;\n\n\n\n4 records\n\n\nID\n\n\n\n\n19991\n\n\n54321\n\n\n55739\n\n\n70557\n\n\n\n\n\n\n\n\n\n\nShow R Code\nSELECT instructor.ID, instructor.name\nFROM instructor\nJOIN teaches ON instructor.ID = teaches.ID\nJOIN course ON teaches.course_id = course.course_id\nGROUP BY instructor.ID, instructor.name, instructor.dept_name\nHAVING COUNT(DISTINCT course.course_id) = \n  (SELECT COUNT(DISTINCT course.course_id)\n   FROM course\n   WHERE course.dept_name = instructor.dept_name)\nORDER BY instructor.name;\n\n\n\n5 records\n\n\nID\nname\n\n\n\n\n22222\nEinstein\n\n\n32343\nEl Said\n\n\n98345\nKim\n\n\n15151\nMozart\n\n\n12121\nWu\n\n\n\n\n\n\n\n\n\n\nShow R Code\nlibrary(RPostgres) \nlibrary(DBI)        \nlibrary(odbc)       \ncon &lt;- dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"university\",   \n  host     = \"localhost\",    \n  port     = 5432,           \n  user     = \"postgres\",     \n  password = \"life123$\"     \n)\n\n## Perform queries\n\n# (a) Simple query: fetch all rows/columns in 'instructor' and create a data object\ninstructor_data &lt;- dbGetQuery(con, \"SELECT * FROM instructor\")\nhead(instructor_data)\n\n\n     id           name   dept_name    salary\n1 63395       McKinnon Cybernetics  94333.99\n2 78699          Pingr  Statistics  59303.62\n3 96895           Mird   Marketing 119921.41\n4  4233            Luo     English  88791.45\n5  4034         Murata   Athletics  61387.56\n6 50885 Konstantinides   Languages  32570.50\n\n\nShow R Code\n# (b) Another query: fetch instructors in 'Comp. Sci.' department \n# with a salary &gt; 60000 (example condition)\ncomp_sci_instructors &lt;- dbGetQuery(\n  con, \n  \"SELECT * FROM instructor \n   WHERE dept_name = 'Comp. Sci.' AND salary &gt; 60000;\"\n)\ncomp_sci_instructors\n\n\n     id     name  dept_name    salary\n1 34175    Bondi Comp. Sci. 115469.11\n2  3335 Bourrier Comp. Sci.  80797.83\n\n\nShow R Code\n# (c) Query a different table, e.g., 'student', and store in an R dataframe\nstudent_data &lt;- dbGetQuery(con, \"SELECT * FROM student WHERE tot_cred &gt;= 50\")\nhead(student_data)\n\n\n     id       name  dept_name tot_cred\n1 79352      Rumat    Finance      100\n2 76672     Miliko Statistics      116\n3 14182 Moszkowski Civil Eng.       73\n4 44985     Prieto    Biology       91\n5 44271    Sowerby    English      108\n6 40897    Coppens       Math       58\n\n\nShow R Code\n## Export to CSV\n\n# Export the entire 'instructor' table (already in instructor_data) to CSV\nwrite.csv(instructor_data, file = \"instructor_export.csv\", row.names = FALSE)\n\n## Clean up\n\n# Always disconnect when done\ndbDisconnect(con)"
  },
  {
    "objectID": "final-im.html",
    "href": "final-im.html",
    "title": "final-im",
    "section": "",
    "text": "Final Project:"
  },
  {
    "objectID": "KM-Project.html",
    "href": "KM-Project.html",
    "title": "KM-Project",
    "section": "",
    "text": "Powerpoint"
  },
  {
    "objectID": "dataframe.html",
    "href": "dataframe.html",
    "title": "Dataframe Review",
    "section": "",
    "text": "Prepare for Class 1 - Dataframe ep 315 Future-Proofing Your Career in AI and Data Analytics with Megan Bowers\nReview:\nFocus on developing skills in advising and monitoring AI systems, as the role of data professionals is shifting towards guiding AI rather than solely coding, especially with AI’s growing capability to generate code.\nPrioritize the creation of a semantic layer within your organization to ensure consistent definitions of business metrics across teams, reducing confusion and improving the reliability of AI-driven insights.\nDevelop a habit of continuous learning by integrating small, regular learning sessions into your routine, and leverage AI tools to efficiently digest and summarize complex research papers and industry developments."
  },
  {
    "objectID": "assignment4-dv.html",
    "href": "assignment4-dv.html",
    "title": "Assignment 4- Data Methods",
    "section": "",
    "text": "Show R Code\n# Load required libraries\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow R Code\nlibrary(rvest)\n\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n\nShow R Code\nlibrary(stringr)\n\n# Define a reusable function to scrape a specific table from a Wikipedia page\nscrape_wiki_table &lt;- function(url, table_index = 1, clean_dates = FALSE) {\n  # Read the HTML content\n  page &lt;- read_html(url)\n  \n  # Extract all tables\n  tables &lt;- page %&gt;% html_nodes(\"table\") %&gt;% html_table(fill = TRUE)\n  \n  # Check if the requested table index exists\n  if (table_index &gt; length(tables)) {\n    stop(\"Requested table index exceeds number of tables found.\")\n  }\n  \n  # Extract the desired table\n  table &lt;- tables[[table_index]]\n  \n  # Optional: Clean up date column if present\n  if (clean_dates && \"Date\" %in% colnames(table)) {\n    table$newdate &lt;- str_split_fixed(table$Date, \"\\\\[\", n = 2)[, 1]\n    table$newdate &lt;- trimws(table$newdate)\n  }\n  \n  return(table)\n}\n\nurl1 &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves\"\nforex_table &lt;- scrape_wiki_table(url1, table_index = 1, clean_dates = TRUE)\n\n# Rename columns (adjust as needed)\nnames(forex_table) &lt;- c(\"Rank\", \"Country\", \"Forexres\", \"Date\", \"Change\", \"Sources\")\n\n\nWarning: The `value` argument of `names&lt;-()` must have the same length as `x` as of\ntibble 3.0.0.\n\n\nWarning: The `value` argument of `names&lt;-()` can't be empty as of tibble 3.0.0.\n\n\nShow R Code\n# View top rows\nhead(forex_table)\n\n\n# A tibble: 6 × 8\n  Rank                         Country Forexres Date  Change Sources ``    ``   \n  &lt;chr&gt;                        &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n1 Country(as recognized by th… Contin… Includi… Incl… Exclu… Exclud… Last… Ref. \n2 Country(as recognized by th… Contin… million… Chan… milli… Change  Last… Ref. \n3 China                        Asia    3,643,1… 41,0… 3,389… 31,221  31 A… [3]  \n4 Japan                        Asia    1,324,2… 19,7… 1,230… 16,230  31 A… [4]  \n5 Switzerland                  Europe  1,007,7… 13,9… 897,2… 14,490  31 J… [5]  \n6 Russia                       Europe… 734,100  14,3… 434,4… 1,517   14 N… [6]  \n\n\n\n\n\n\n\nShow R Code\nclean_data &lt;- forex_table[-c(1, 2), ] %&gt;%\n  select(Country, Forexres, Date) %&gt;%\n  mutate(\n    Country = str_remove(trimws(Country), \"\\\\[.*\\\\]\"),\n    Forexres = as.numeric(gsub(\",\", \"\", Forexres)),\n    Date = str_split_fixed(Date, \"\\\\[\", 2)[, 1] %&gt;% trimws()\n  )\n\n\n\n\n\n\n\nDefine the research objectives\nIdentify Data Sources\nAssess Accessability and Permissions\nDesign Scraping Logic\nClean and Validate Data\nStore Data\nAnalyze and Visualize Data\nMonitor and Maintain\nDocument the Process\nEnsure Ethical Compliance\nReview and Iterate\nIntegrate with Analysis pipeline"
  },
  {
    "objectID": "assignment5-dv.html",
    "href": "assignment5-dv.html",
    "title": "Assignment - 5",
    "section": "",
    "text": "Show R Code\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\n\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  830981 44.4    1598564 85.4   830981 44.4\nVcells 1648985 12.6    8388608 64.0  1648985 12.6\n\n\nShow R Code\n# install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nShow R Code\n## Set path for reading the listing and home directory\n\n\nsetwd(\"C:\\\\Users\\\\Niha\\\\OneDrive\\\\Documents\\\\Grad School\\\\nihay17.github.io\\\\files\")\nlibrary(rjson)\nlibrary(jsonlite)\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nShow R Code\nlibrary(data.table)\n\n\n\nAttaching package: 'data.table'\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nShow R Code\nlibrary(readr)\n\n## CSV method\ngovfiles= read.csv(file=\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\", skip=2)\n\n## JSON method\n### rjson\ngf_list &lt;- rjson::fromJSON(file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfile2=dplyr::bind_rows(gf_list$resultSet)\n\n### jsonlite\ngf_list1 = jsonlite::read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\n\n### Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n### One more step\ngovfiles3 &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\n\n# Preparing for bulk download of government documents\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles3$pdfLink\npdf_govfiles_id &lt;- govfiles3$index\n\n# Directory to save the pdf's\n# Be sure to create a folder for storing the pdf's\nsave_dir &lt;- \"C:\\\\Users\\\\Niha\\\\OneDrive\\\\Documents\\\\Grad School\\\\nihay17.github.io\\\\files\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n\n## Try ten\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\n\n\nStarting downloads\n\n\nShow R Code\nresults &lt;- 1:10 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\n\n\nFinished downloads\n\n\nShow R Code\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\n\nTime difference of 23.47516 secs\n\n\nShow R Code\n# Print results\nprint(results)\n\n\n [1] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sres890is/pdf/BILLS-118sres890is.pdf\"        \n [2] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres114is/pdf/BILLS-118sjres114is.pdf\"      \n [3] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sres805ats/pdf/BILLS-118sres805ats.pdf\"      \n [4] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres115is/pdf/BILLS-118sjres115is.pdf\"      \n [5] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres113is/pdf/BILLS-118sjres113is.pdf\"      \n [6] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres111is/pdf/BILLS-118sjres111is.pdf\"      \n [7] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/BILLS-118sjres112is/pdf/BILLS-118sjres112is.pdf\"      \n [8] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgD951.pdf\"   \n [9] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgS6417-2.pdf\"\n[10] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CREC-2024-09-25/pdf/CREC-2024-09-25-pt1-PgS6418.pdf\"  \n\n\nThe above code can be modified to download all files by changing the 1:10 to 1:length(pdf_govfiles_url). Because of this the scraped data can be useable but since it updates frequently it can be improved by using an API from the government website. I had to stop downloads at 10 because it ook up alot of storage and time. The code can be improved by adding a progress bar to show the progress of the downloads. Additionally, error handling can be added to retry failed downloads or log them for later review."
  },
  {
    "objectID": "assignment6-dv.html",
    "href": "assignment6-dv.html",
    "title": "Assignment - 6",
    "section": "",
    "text": "Show R Code\nlibrary(quanteda)\n\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 8 of 8 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nShow R Code\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow R Code\nView(summit)\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\nclass(toks)\n\n\n[1] \"tokens\"\n\n\nShow R Code\n# Latent Semantic Analysis \n## (https://quanteda.io/reference/textmodel_lsa.html)\n\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm, nd=4,  margin = c(\"both\", \"documents\", \"features\"))\nsummary(sum_lsa)\n\n\n                Length    Class     Mode   \nsk                      4 -none-    numeric\ndocs                58080 -none-    numeric\nfeatures            63972 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\n\nShow R Code\nhead(sum_lsa$docs)\n\n\n              [,1]          [,2]          [,3]          [,4]\ntext1 8.678102e-03  9.529008e-03 -3.178574e-03  1.380732e-02\ntext2 8.676818e-06 -8.806186e-06 -5.989637e-06  1.677631e-05\ntext3 2.922127e-03  6.778967e-03  1.131673e-03 -3.176902e-03\ntext4 1.046624e-02  8.884054e-04 -4.282723e-03  4.960680e-03\ntext5 3.251208e-03  8.005843e-03  2.208204e-04 -4.656367e-03\ntext6 3.251208e-03  8.005843e-03  2.208204e-04 -4.656367e-03\n\n\nShow R Code\nclass(sum_lsa)\n\n\n[1] \"textmodel_lsa\"\n\n\nShow R Code\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\n\nShow R Code\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\n\nShow R Code\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\n\nShow R Code\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\nShow R Code\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\n\nShow R Code\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_nfeat ... 10 more features, reached max_nfeat ... 701 more features ]\n\n\nShow R Code\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\n\n\nShow R Code\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\n\n\nShow R Code\ninaug_speech = data_corpus_inaugural\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\n\n\n\n\n\nShow R Code\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\n\n\nShow R Code\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\n\n\nShow R Code\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n  \n)\n\n\n\n\n\n\n\n\n\nShow R Code\n## Why is the \"communist\" plot missing?\n\ntheme_set(theme_bw())\ng &lt;- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nShow R Code\nlibrary(quanteda.textstats)\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by reverse frequency order\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\nShow R Code\n# Get frequency grouped by president\nfreq_grouped &lt;- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\n# Filter the term \"american\"\nfreq_american &lt;- subset(freq_grouped, freq_grouped$feature %in% \"american\")  \n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 14), breaks = c(seq(0, 14, 2))) +\n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nShow R Code\ndfm_rel_freq &lt;- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\nhead(dfm_rel_freq)\n\n\nDocument-feature matrix of: 6 documents, 4,625 features (86.44% sparse) and 4 docvars.\n                 features\ndocs                      my    friends        ,    before          i\n  1953-Eisenhower 0.14582574 0.14582574 4.593511 0.1822822 0.10936930\n  1957-Eisenhower 0.20975354 0.10487677 6.345045 0.1573152 0.05243838\n  1961-Kennedy    0.19467878 0.06489293 5.451006 0.1297859 0.32446463\n  1965-Johnson    0.17543860 0.05847953 5.555556 0.2339181 0.87719298\n  1969-Nixon      0.28973510 0          5.546358 0.1241722 0.86920530\n  1973-Nixon      0.05012531 0.05012531 4.812030 0.2005013 0.60150376\n                 features\ndocs                   begin      the expression       of     those\n  1953-Eisenhower 0.03645643 6.234050 0.03645643 5.176814 0.1458257\n  1957-Eisenhower 0          5.977976 0          5.034085 0.1573152\n  1961-Kennedy    0.19467878 5.580792 0          4.218040 0.4542505\n  1965-Johnson    0          4.502924 0          3.333333 0.1754386\n  1969-Nixon      0          5.629139 0          3.890728 0.4552980\n  1973-Nixon      0          4.160401 0          3.408521 0.3007519\n[ reached max_nfeat ... 4,615 more features ]\n\n\nShow R Code\nrel_freq &lt;- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\n\n# Filter the term \"american\"\nrel_freq_american &lt;- subset(rel_freq, feature %in% \"american\")  \n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 0.7), breaks = c(seq(0, 0.7, 0.1))) +\n  xlab(NULL) + \n  ylab(\"Relative frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nShow R Code\ndfm_weight_pres &lt;- data_corpus_inaugural %&gt;%\n  corpus_subset(Year &gt; 2000) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by president\nfreq_weight &lt;- textstat_frequency(dfm_weight_pres, n = 10, \n                                  groups = dfm_weight_pres$President)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n  geom_point() +\n  facet_wrap(~ group, scales = \"free\") +\n  coord_flip() +\n  scale_x_continuous(breaks = nrow(freq_weight):1,\n                     labels = freq_weight$feature) +\n  labs(x = NULL, y = \"Relative frequency\")\n\n\n\n\n\n\n\n\n\nShow R Code\n# Only select speeches by Obama and Trump\npres_corpus &lt;- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\n# Create a dfm grouped by president\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_group(groups = President) %&gt;%\n  dfm()\n\n# Calculate keyness and determine Trump as target group\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness) \n\n\n\n\n\n\n\n\n\nShow R Code\n# Plot without the reference text (in this case Obama)\ntextplot_keyness(result_keyness, show_reference = FALSE)\n\n\n\n\n\n\n\n\n\nShow R Code\nlibrary(quanteda.textmodels)\n\n# Irish budget speeches from 2010 (data from quanteda.textmodels)\n# Transform corpus to dfm\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Predict Wordscores model\nws &lt;- textmodel_wordscores(ie_dfm, y = refscores, smooth = 1)\n\n# Plot estimated word positions (highlight words and print them in red)\ntextplot_scale1d(ws,\n                 highlighted = c(\"minister\", \"have\", \"our\", \"budget\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n\n\nShow R Code\n# Get predictions\npred &lt;- predict(ws, se.fit = TRUE)\n\n# Plot estimated document positions and group by \"party\" variable\ntextplot_scale1d(pred, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\nShow R Code\n# Plot estimated document positions using the LBG transformation and group by \"party\" variable\n\npred_lbg &lt;- predict(ws, se.fit = TRUE, rescaling = \"lbg\")\n\ntextplot_scale1d(pred_lbg, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\nShow R Code\n# Estimate Wordfish model\nlibrary(\"quanteda.textmodels\")\nwf &lt;- textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)), dir = c(6, 5))\n\n# Plot estimated word positions\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n\n\nShow R Code\n# Plot estimated document positions\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n\n\nShow R Code\n# Transform corpus to dfm\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Run correspondence analysis on dfm\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot estimated positions and group by party\ntextplot_scale1d(ca, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\n3.The similarities that each president shares based on their inaugural addresses can be analyzed using various text analysis techniques such as Latent Semantic Analysis (LSA), word clouds, and keyness analysis.\n\nWordfish is a quantitative text analysis method used in political science and social sciences to estimate the ideological positions of texts or speakers based on word frequencies."
  }
]